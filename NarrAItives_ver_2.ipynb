{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKd4YYl0UFWhgMRSlpJkqR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KOOLPLUG/NarrAItives/blob/main/NarrAItives_ver_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5K3lRBObSce"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "from newspaper import Article\n",
        "\n",
        "# -----------------------\n",
        "# 1. SETUP\n",
        "# -----------------------\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    return pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Full Rhetoric Narration Bins\n",
        "rhetoric_bins = [\n",
        "    \"Us vs Them: Frames one group as morally superior and the other as dangerous, inferior, or untrustworthy.\",\n",
        "    \"Exceptionalism: Claims a nation or group is unique, morally superior, or destined for a special role in the world.\",\n",
        "    \"Security Threat Inflation: Exaggerates or amplifies the scale of a threat to justify urgent or extreme action.\",\n",
        "    \"Humanitarian Pretext: Presents intervention or policy as purely altruistic and compassionate, masking strategic goals.\",\n",
        "    \"Moral Panic / Outrage: Focuses on moral or ethical violations to spark strong emotional reactions in the public.\",\n",
        "    \"Victimhood / Persecution Narrative: Portrays own group as unfairly targeted, oppressed, or under attack.\",\n",
        "    \"Destiny & Progress: Frames events as part of inevitable historical progress or being on the 'right side of history.'\",\n",
        "    \"Unity Against a Common Enemy: Calls for cohesion and solidarity by identifying and opposing a shared adversary.\"\n",
        "]\n",
        "\n",
        "# Rule-based mapping for appeals & audiences\n",
        "appeal_mapping = {\n",
        "    \"Us vs Them\": (\"Identity Appeal\", \"Nationalists / Patriots\"),\n",
        "    \"Exceptionalism\": (\"Identity Appeal\", \"Nationalists / Patriots\"),\n",
        "    \"Security Threat Inflation\": (\"Fear/Security Appeal\", \"Security-Conscious Citizens\"),\n",
        "    \"Humanitarian Pretext\": (\"Compassion Appeal\", \"Humanitarians / Global Justice Advocates\"),\n",
        "    \"Moral Panic / Outrage\": (\"Emotional Appeal\", \"General Public / Concerned Citizens\"),\n",
        "    \"Victimhood / Persecution Narrative\": (\"Victimhood Appeal\", \"Marginalized or Supportive Groups\"),\n",
        "    \"Destiny & Progress\": (\"Progress Appeal\", \"Progress-Oriented Groups\"),\n",
        "    \"Unity Against a Common Enemy\": (\"Unity Appeal\", \"General Public / Allies\")\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# SIDEBAR NAVIGATION\n",
        "# -----------------------\n",
        "st.sidebar.title(\"NarrAItives\")\n",
        "page = st.sidebar.radio(\"Select a page:\", [\"Detect Rhetoric\", \"Coming Soon...\"])\n",
        "\n",
        "# -----------------------\n",
        "# DETECT RHETORIC PAGE\n",
        "# -----------------------\n",
        "if page == \"Detect Rhetoric\":\n",
        "    st.title(\"ðŸ“° Detect Rhetoric in News Articles\")\n",
        "    st.write(\"Paste a news article URL to see the detected rhetoric, intended appeal, and target audience.\")\n",
        "\n",
        "    url = st.text_input(\"Enter a news article URL:\")\n",
        "\n",
        "    if url:\n",
        "        try:\n",
        "            # Step 2: Fetch article text\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            text = article.text.strip()\n",
        "\n",
        "            if text:\n",
        "                # Step 3: Zero-shot classification\n",
        "                with st.spinner(\"Analyzing article...\"):\n",
        "                    result = classifier(text, rhetoric_bins, multi_label=True)\n",
        "\n",
        "                # Get top rhetoric\n",
        "                top_label_full = result[\"labels\"][0]\n",
        "                top_score = result[\"scores\"][0]\n",
        "\n",
        "                # Extract the short label (before \":\")\n",
        "                top_label_short = top_label_full.split(\":\")[0]\n",
        "\n",
        "                # Step 4: Audience & Appeal Analysis\n",
        "                appeal_type, target_audience = appeal_mapping.get(\n",
        "                    top_label_short, (\"Unknown\", \"Unknown\")\n",
        "                )\n",
        "\n",
        "                # Step 5: Output\n",
        "                st.subheader(\"Analysis Summary\")\n",
        "                st.write(f\"**Primary Rhetoric:** {top_label_full} ({top_score:.2f})\")\n",
        "                st.write(f\"**Primary Appeal:** {appeal_type}\")\n",
        "                st.write(f\"**Likely Target Audience:** {target_audience}\")\n",
        "\n",
        "                st.markdown(\"---\")\n",
        "                st.subheader(\"Full Rhetoric Scores\")\n",
        "                for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
        "                    st.write(f\"{label}: {score:.2f}\")\n",
        "\n",
        "            else:\n",
        "                st.error(\"Could not extract any text from this URL.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing URL: {e}\")\n",
        "\n",
        "# -----------------------\n",
        "# PLACEHOLDER FOR FUTURE PAGES\n",
        "# -----------------------\n",
        "elif page == \"Coming Soon...\":\n",
        "    st.title(\"More features coming soon!\")\n",
        "    st.write(\"Stay tuned for additional tools to analyze media narratives.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: Libraries & API Configuration"
      ],
      "metadata": {
        "id": "QqeoTdobf7Jw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e5429d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a4d8e1-2fdf-48db-cc4b-6f4737a48010"
      },
      "source": [
        "# Install necessary libraries\n",
        "%pip install streamlit newsapi-python transformers matplotlib pandas"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9488d4ed"
      },
      "source": [
        "Next, you'll need to set up a Hugging Face Space. This is where your Streamlit app will be hosted.\n",
        "\n",
        "1.  Go to the [Hugging Face Spaces website](https://huggingface.co/spaces).\n",
        "2.  Click on \"Create new Space\".\n",
        "3.  Choose a name for your Space.\n",
        "4.  Select \"Streamlit\" as the Space SDK.\n",
        "5.  Choose a repository (e.g., a new repository).\n",
        "6.  Click \"Create Space\".\n",
        "\n",
        "Once your Space is created, you'll need to connect it to your local environment or upload your app files directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1eead23"
      },
      "source": [
        "You'll also need to obtain API keys for the news API you plan to use. Many news APIs require registration and provide an API key to access their data. Follow the documentation of your chosen news API to get your key.\n",
        "\n",
        "Once you have your API key, it's recommended to store it securely. In Colab, you can use the secrets manager (the \"ðŸ”‘\" icon in the left panel) to store your API key and access it in your code without exposing it directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f44c9d2"
      },
      "source": [
        "Here's a basic structure for your Streamlit app (`app.py`) that you'll place in your Hugging Face Space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e339a985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd45890-14e3-44c6-8fb2-c7c7263702d4"
      },
      "source": [
        "# app.py\n",
        "import streamlit as st\n",
        "# You'll need to import the news API library you chose and potentially other libraries here\n",
        "# e.g., from newsapi import NewsApiClient\n",
        "# from transformers import pipeline\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "from google.colab import userdata\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "# Example:\n",
        "# search_query = st.text_input(\"Enter a news search query:\")\n",
        "# if st.button(\"Analyze\"):\n",
        "#     # Fetch news articles using the news API\n",
        "#     # Process articles with a rhetoric detection model (e.g., from transformers)\n",
        "#     # Display results, potentially with visualizations\n",
        "\n",
        "# You'll need to add your news API key and potentially Hugging Face model loading logic here\n",
        "# Example:\n",
        "# newsapi = NewsApiClient(api_key=st.secrets[\"NEWS_API_KEY\"])\n",
        "# classifier = pipeline(\"text-classification\", model=\"your-rhetoric-detection-model\")\n",
        "\n",
        "# Access the news API key from Colab secrets\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "# Now you can use news_api_key in your code to initialize your news API client\n",
        "# Example:\n",
        "# newsapi = NewsApiClient(api_key=news_api_key)\n",
        "\n",
        "\n",
        "# Add code to generate plots if needed\n",
        "# Example:\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-05 15:05:07.763 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:07.975 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-08-05 15:05:07.976 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:07.977 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c35a6a"
      },
      "source": [
        "Remember to replace the placeholder comments and code in `app.py` with your actual implementation for fetching news, performing rhetoric detection, and generating visualizations.\n",
        "\n",
        "You can then push your `app.py` file to your Hugging Face Space repository or upload it directly through the Hugging Face website."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dd3d888",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad52b0f-7c10-4ef5-f51a-970728752185"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "newsapi-python\n",
        "transformers\n",
        "matplotlib\n",
        "pandas"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b29adb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ecd6ab-0806-4f84-8251-b0268d0515de"
      },
      "source": [
        "# Example of fetching news in app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Access the news API key from Colab secrets\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "# Initialize the News API client\n",
        "# Ensure you have stored your API key in Colab secrets as 'newsapi'\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop() # Stop the app if the API key is not available\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            # Fetch top headlines or articles based on the query\n",
        "            # You can adjust parameters like sources, domains, language, etc.\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                # Process each article (you'll add your rhetoric detection here)\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    # Add your rhetoric detection model processing here\n",
        "                    # rhetoric_score = your_rhetoric_model(article['content'])\n",
        "                    # st.write(f\"Rhetoric Score: {rhetoric_score}\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query.\")\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# from transformers import pipeline\n",
        "# classifier = pipeline(\"text-classification\", model=\"your-rhetoric-detection-model\")\n",
        "# def analyze_rhetoric(text):\n",
        "#     # Implement rhetoric analysis using the classifier\n",
        "#     pass # Replace with your analysis logic\n",
        "\n",
        "# Add code to generate plots if needed\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-05 15:05:08.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:08.815 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:08.819 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.994 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.995 Session state does not function when running a script without `streamlit run`\n",
            "2025-08-05 15:05:09.997 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:09.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.001 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.003 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.012 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-05 15:05:10.013 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")"
      ],
      "metadata": {
        "id": "dBHFLnCEhj5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cb0591-d726-46f9-a99a-9e0a66ddb8e7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cdf4619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "outputId": "5283d73d-540d-4708-9c84-1f59ea27d401"
      },
      "source": [
        "!pip install newspaper3k lxml_html_clean\n",
        "from newspaper import Article\n",
        "\n",
        "url = \"https://example.com/news-article\"\n",
        "article = Article(url)\n",
        "article.download()\n",
        "article.parse()\n",
        "print(article.text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.7.14)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ArticleException",
          "evalue": "Article `download()` failed with 404 Client Error: Not Found for url: https://example.com/news-article on URL https://example.com/news-article",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArticleException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3294811755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow_if_not_downloaded_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36mthrow_if_not_downloaded_verbose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mArticleException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must `download()` an article first!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             raise ArticleException('Article `download()` failed with %s on URL %s' %\n\u001b[0m\u001b[1;32m    532\u001b[0m                   (self.download_exception_msg, self.url))\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mArticleException\u001b[0m: Article `download()` failed with 404 Client Error: Not Found for url: https://example.com/news-article on URL https://example.com/news-article"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rhetoric Detection URL"
      ],
      "metadata": {
        "id": "3B4gfAHHjHGe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbee81d9"
      },
      "source": [
        "# Task\n",
        "Implement the first feature of the rhetoric detection MVP app in `app.py` and `requirements.txt`. This feature should take a URL as input, fetch and parse the article text using `newspaper3k`, perform zero-shot classification against the provided rhetoric narration and appeal type bins, infer the probable target audience based on the results, and display the analysis in a clear summary format. Ensure all necessary libraries are included in `requirements.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddbacc4"
      },
      "source": [
        "## Modify `app.py` to accept url input\n",
        "\n",
        "### Subtask:\n",
        "Update the Streamlit code to include a text input field specifically for a URL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61e4f51"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `app.py` file to include a text input for the URL and write the updated content to the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4c08ef7"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# You'll need to import the news API library you chose and potentially other libraries here\n",
        "# e.g., from newsapi import NewsApiClient\n",
        "# from transformers import pipeline\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "\n",
        "# Access the news API key from Colab secrets\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "# Initialize the News API client\n",
        "# Ensure you have stored your API key in Colab secrets as 'newsapi'\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop() # Stop the app if the API key is not available\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            # Fetch top headlines or articles based on the query\n",
        "            # You can adjust parameters like sources, domains, language, etc.\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                # Process each article (you'll add your rhetoric detection here)\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    # Add your rhetoric detection model processing here\n",
        "                    # rhetoric_score = your_rhetoric_model(article['content'])\n",
        "                    # st.write(f\"Rhetoric Score: {rhetoric_score}\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query.\")\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# from transformers import pipeline\n",
        "# classifier = pipeline(\"text-classification\", model=\"your-rhetoric-detection-model\")\n",
        "# def analyze_rhetoric(text):\n",
        "#     # Implement rhetoric analysis using the classifier\n",
        "#     pass # Replace with your analysis logic\n",
        "\n",
        "# Add code to generate plots if needed\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e512984c"
      },
      "source": [
        "## Add `newspaper3k` integration\n",
        "\n",
        "### Subtask:\n",
        "Include the necessary imports and code to use `newspaper3k` to download and parse the article content from the provided URL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f975bf05"
      },
      "source": [
        "**Reasoning**:\n",
        "Include the necessary imports and code to use `newspaper3k` to download and parse the article content from the provided URL in `app.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0390d0e2"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "from newspaper import Article\n",
        "\n",
        "# You'll need to import the news API library you chose and potentially other libraries here\n",
        "# e.g., from newsapi import NewsApiClient\n",
        "# from transformers import pipeline\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "\n",
        "# Access the news API key from Colab secrets\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "# Initialize the News API client\n",
        "# Ensure you have stored your API key in Colab secrets as 'newsapi'\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop() # Stop the app if the API key is not available\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            # Fetch top headlines or articles based on the query\n",
        "            # You can adjust parameters like sources, domains, language, etc.\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                # Process each article (you'll add your rhetoric detection here)\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    # Add your rhetoric detection model processing here\n",
        "                    # rhetoric_score = your_rhetoric_model(article['content'])\n",
        "                    # st.write(f\"Rhetoric Score: {rhetoric_score}\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    elif article_url:\n",
        "        try:\n",
        "            # Use newspaper3k to download and parse the article\n",
        "            article = Article(article_url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            st.subheader(\"Article Content:\")\n",
        "            st.write(article.text) # Display the extracted text\n",
        "\n",
        "            # You'll add your rhetoric detection model processing here\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while processing the URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query or an article URL.\")\n",
        "\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# from transformers import pipeline\n",
        "# classifier = pipeline(\"text-classification\", model=\"your-rhetoric-detection-model\")\n",
        "# def analyze_rhetoric(text):\n",
        "#     # Implement rhetoric analysis using the classifier\n",
        "#     pass # Replace with your analysis logic\n",
        "\n",
        "# Add code to generate plots if needed\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d953083"
      },
      "source": [
        "## Implement rhetoric narration classification\n",
        "\n",
        "### Subtask:\n",
        "Integrate the zero-shot classification pipeline and use the provided `rhetoric_bins` to classify the extracted article text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7f1e55d"
      },
      "source": [
        "**Reasoning**:\n",
        "Add imports for the zero-shot classification pipeline, define the rhetoric bins, and load the pipeline and tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e93f9253"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "from newspaper import Article\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Define rhetoric bins\n",
        "rhetoric_bins = [\n",
        "    \"logical appeal\",\n",
        "    \"emotional appeal\",\n",
        "    \"ethical appeal\",\n",
        "    \"personal anecdote\",\n",
        "    \"statistical evidence\",\n",
        "    \"expert opinion\",\n",
        "    \"loaded language\",\n",
        "    \"rhetorical question\",\n",
        "    \"call to action\"\n",
        "]\n",
        "\n",
        "# Load zero-shot classification pipeline\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
        "    return classifier\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop()\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    elif article_url:\n",
        "        try:\n",
        "            article = Article(article_url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            st.subheader(\"Article Content:\")\n",
        "            st.write(article.text)\n",
        "\n",
        "            # Classify the article text using the zero-shot classification pipeline\n",
        "            classification_results = classifier(article.text, rhetoric_bins, multi_label=True)\n",
        "\n",
        "            # Store the classification results for later display\n",
        "            st.session_state['classification_results'] = classification_results\n",
        "\n",
        "            # Placeholder for displaying results - will be implemented in the next step\n",
        "            st.subheader(\"Rhetoric Analysis Results (Coming Soon):\")\n",
        "            st.write(\"Analysis will be displayed here in the next step.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while processing the URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query or an article URL.\")\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1d1bf64"
      },
      "source": [
        "## Implement appeal type classification\n",
        "\n",
        "### Subtask:\n",
        "Use the zero-shot classification pipeline and the provided appeal type categories to classify the extracted article text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fdf7296"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the appeal types and use the zero-shot classification pipeline to classify the article text against them, storing the results in session state.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301806c0"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "from newspaper import Article\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Define rhetoric bins\n",
        "rhetoric_bins = [\n",
        "    \"logical appeal\",\n",
        "    \"emotional appeal\",\n",
        "    \"ethical appeal\",\n",
        "    \"personal anecdote\",\n",
        "    \"statistical evidence\",\n",
        "    \"expert opinion\",\n",
        "    \"loaded language\",\n",
        "    \"rhetorical question\",\n",
        "    \"call to action\"\n",
        "]\n",
        "\n",
        "# Define appeal types\n",
        "appeal_types = ['Ethos', 'Pathos', 'Logos']\n",
        "\n",
        "\n",
        "# Load zero-shot classification pipeline\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
        "    return classifier\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop()\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    elif article_url:\n",
        "        try:\n",
        "            article = Article(article_url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            st.subheader(\"Article Content:\")\n",
        "            st.write(article.text)\n",
        "\n",
        "            # Classify the article text using the zero-shot classification pipeline\n",
        "            classification_results = classifier(article.text, rhetoric_bins, multi_label=True)\n",
        "\n",
        "            # Store the classification results for later display\n",
        "            st.session_state['classification_results'] = classification_results\n",
        "\n",
        "            # Classify the article text for appeal types\n",
        "            appeal_classification_results = classifier(article.text, appeal_types, multi_label=True)\n",
        "\n",
        "            # Store the appeal type classification results\n",
        "            st.session_state['appeal_classification_results'] = appeal_classification_results\n",
        "\n",
        "            # Placeholder for displaying results - will be implemented in the next step\n",
        "            st.subheader(\"Rhetoric Analysis Results (Coming Soon):\")\n",
        "            st.write(\"Analysis will be displayed here in the next step.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while processing the URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query or an article URL.\")\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50d292af"
      },
      "source": [
        "## Implement rule-based audience & appeal inference\n",
        "\n",
        "### Subtask:\n",
        "Add logic to infer the probable target audience based on the detected rhetoric narration and appeal types using the provided categories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ffee960"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic to infer the probable target audience and probable appeal based on the classification results stored in `st.session_state`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4f84351"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "from newspaper import Article\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Define rhetoric bins\n",
        "rhetoric_bins = [\n",
        "    \"logical appeal\",\n",
        "    \"emotional appeal\",\n",
        "    \"ethical appeal\",\n",
        "    \"personal anecdote\",\n",
        "    \"statistical evidence\",\n",
        "    \"expert opinion\",\n",
        "    \"loaded language\",\n",
        "    \"rhetorical question\",\n",
        "    \"call to action\"\n",
        "]\n",
        "\n",
        "# Define appeal types\n",
        "appeal_types = ['Ethos', 'Pathos', 'Logos']\n",
        "\n",
        "\n",
        "# Load zero-shot classification pipeline\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
        "    return classifier\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop()\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    elif article_url:\n",
        "        try:\n",
        "            article = Article(article_url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            st.subheader(\"Article Content:\")\n",
        "            st.write(article.text)\n",
        "\n",
        "            # Classify the article text using the zero-shot classification pipeline\n",
        "            classification_results = classifier(article.text, rhetoric_bins, multi_label=True)\n",
        "            st.session_state['classification_results'] = classification_results\n",
        "\n",
        "            # Classify the article text for appeal types\n",
        "            appeal_classification_results = classifier(article.text, appeal_types, multi_label=True)\n",
        "            st.session_state['appeal_classification_results'] = appeal_classification_results\n",
        "\n",
        "            # Infer probable target audience and appeal\n",
        "            rhetoric_df = pd.DataFrame(classification_results)\n",
        "            appeal_df = pd.DataFrame(appeal_classification_results)\n",
        "\n",
        "            # Simple inference logic: find the top rhetoric and appeal types\n",
        "            top_rhetoric = rhetoric_df.loc[rhetoric_df['scores'].idxmax()]\n",
        "            top_appeal = appeal_df.loc[appeal_df['scores'].idxmax()]\n",
        "\n",
        "            inferred_audience = \"General Audience\"\n",
        "            probable_appeal = \"Informative\"\n",
        "\n",
        "            if top_appeal['label'] == 'Pathos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Emotionally-driven individuals\"\n",
        "                probable_appeal = \"Emotional\"\n",
        "            elif top_appeal['label'] == 'Logos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Individuals who value logic and evidence\"\n",
        "                probable_appeal = \"Rational\"\n",
        "            elif top_appeal['label'] == 'Ethos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Individuals who respect authority and credibility\"\n",
        "                probable_appeal = \"Ethical/Credible\"\n",
        "\n",
        "            if top_rhetoric['label'] == 'statistical evidence' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals who value data and facts\"\n",
        "                 probable_appeal = \"Data-driven\"\n",
        "            elif top_rhetoric['label'] == 'expert opinion' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals who trust expert endorsements\"\n",
        "                 probable_appeal = \"Expert-backed\"\n",
        "            elif top_rhetoric['label'] == 'loaded language' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals susceptible to biased language\"\n",
        "                 probable_appeal = \"Persuasive/Biased\"\n",
        "            elif top_rhetoric['label'] == 'call to action' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals motivated to take action\"\n",
        "                 probable_appeal = \"Action-oriented\"\n",
        "\n",
        "\n",
        "            st.session_state['inferred_audience'] = inferred_audience\n",
        "            st.session_state['probable_appeal'] = probable_appeal\n",
        "\n",
        "            st.subheader(\"Rhetoric Analysis Results:\")\n",
        "            st.write(\"Analysis will be displayed here in the next step.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while processing the URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query or an article URL.\")\n",
        "\n",
        "# You'll also need to add your rhetoric detection model loading and processing logic,\n",
        "# and code to generate visualizations if needed.\n",
        "# Example:\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.bar(['Positive', 'Negative'], [pos_count, neg_count])\n",
        "# st.pyplot(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98df1615"
      },
      "source": [
        "## Format and display output\n",
        "\n",
        "### Subtask:\n",
        "Structure the analysis results in the clear, short summary format you provided earlier and display it in the Streamlit app.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a01af627"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the display of the analysis results in the specified clear and short summary format within the Streamlit app, including the extracted text, inferred audience and appeal, and detailed classification scores for both rhetoric narration and appeal types.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ec1b1d1"
      },
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "from newsapi import NewsApiClient\n",
        "from google.colab import userdata\n",
        "from newspaper import Article\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "\n",
        "st.title(\"Rhetoric Detection MVP\")\n",
        "\n",
        "# Define rhetoric bins\n",
        "rhetoric_bins = [\n",
        "    \"logical appeal\",\n",
        "    \"emotional appeal\",\n",
        "    \"ethical appeal\",\n",
        "    \"personal anecdote\",\n",
        "    \"statistical evidence\",\n",
        "    \"expert opinion\",\n",
        "    \"loaded language\",\n",
        "    \"rhetorical question\",\n",
        "    \"call to action\"\n",
        "]\n",
        "\n",
        "# Define appeal types\n",
        "appeal_types = ['Ethos', 'Pathos', 'Logos']\n",
        "\n",
        "\n",
        "# Load zero-shot classification pipeline\n",
        "@st.cache_resource\n",
        "def load_classifier():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\")\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
        "    return classifier\n",
        "\n",
        "classifier = load_classifier()\n",
        "\n",
        "# Add input fields, buttons, and display areas for your app\n",
        "news_api_key = userdata.get(\"newsapi\")\n",
        "\n",
        "if news_api_key:\n",
        "    newsapi = NewsApiClient(api_key=news_api_key)\n",
        "else:\n",
        "    st.error(\"News API key not found. Please add it to Colab secrets named 'newsapi'.\")\n",
        "    st.stop()\n",
        "\n",
        "search_query = st.text_input(\"Enter a news search query:\")\n",
        "article_url = st.text_input(\"Enter news article URL:\")\n",
        "\n",
        "if st.button(\"Analyze\"):\n",
        "    if search_query:\n",
        "        try:\n",
        "            articles = newsapi.get_everything(q=search_query, language='en', sort_by='relevancy')\n",
        "\n",
        "            if articles and articles['articles']:\n",
        "                st.write(f\"Found {articles['totalResults']} articles for '{search_query}':\")\n",
        "\n",
        "                for article in articles['articles']:\n",
        "                    st.subheader(article['title'])\n",
        "                    st.write(article['description'])\n",
        "                    st.markdown(f\"[Read more]({article['url']})\")\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "            else:\n",
        "                st.write(\"No articles found for this query.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred: {e}\")\n",
        "    elif article_url:\n",
        "        try:\n",
        "            article = Article(article_url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            # Classify the article text using the zero-shot classification pipeline\n",
        "            classification_results = classifier(article.text, rhetoric_bins, multi_label=True)\n",
        "            st.session_state['classification_results'] = classification_results\n",
        "\n",
        "            # Classify the article text for appeal types\n",
        "            appeal_classification_results = classifier(article.text, appeal_types, multi_label=True)\n",
        "            st.session_state['appeal_classification_results'] = appeal_classification_results\n",
        "\n",
        "            # Infer probable target audience and appeal\n",
        "            rhetoric_df = pd.DataFrame(classification_results)\n",
        "            appeal_df = pd.DataFrame(appeal_classification_results)\n",
        "\n",
        "            # Simple inference logic: find the top rhetoric and appeal types\n",
        "            top_rhetoric = rhetoric_df.loc[rhetoric_df['scores'].idxmax()]\n",
        "            top_appeal = appeal_df.loc[appeal_df['scores'].idxmax()]\n",
        "\n",
        "            inferred_audience = \"General Audience\"\n",
        "            probable_appeal = \"Informative\"\n",
        "\n",
        "            if top_appeal['label'] == 'Pathos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Emotionally-driven individuals\"\n",
        "                probable_appeal = \"Emotional\"\n",
        "            elif top_appeal['label'] == 'Logos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Individuals who value logic and evidence\"\n",
        "                probable_appeal = \"Rational\"\n",
        "            elif top_appeal['label'] == 'Ethos' and top_appeal['scores'] > 0.7:\n",
        "                inferred_audience = \"Individuals who respect authority and credibility\"\n",
        "                probable_appeal = \"Ethical/Credible\"\n",
        "\n",
        "            if top_rhetoric['label'] == 'statistical evidence' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals who value data and facts\"\n",
        "                 probable_appeal = \"Data-driven\"\n",
        "            elif top_rhetoric['label'] == 'expert opinion' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals who trust expert endorsements\"\n",
        "                 probable_appeal = \"Expert-backed\"\n",
        "            elif top_rhetoric['label'] == 'loaded language' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals susceptible to biased language\"\n",
        "                 probable_appeal = \"Persuasive/Biased\"\n",
        "            elif top_rhetoric['label'] == 'call to action' and top_rhetoric['scores'] > 0.7:\n",
        "                 inferred_audience = \"Individuals motivated to take action\"\n",
        "                 probable_appeal = \"Action-oriented\"\n",
        "\n",
        "\n",
        "            st.session_state['inferred_audience'] = inferred_audience\n",
        "            st.session_state['probable_appeal'] = probable_appeal\n",
        "\n",
        "            # Display analysis results\n",
        "            st.subheader(\"Rhetoric Analysis Summary:\")\n",
        "            st.write(\"---\")\n",
        "            st.write(\"**Extracted Article Text:**\")\n",
        "            st.write(article.text)\n",
        "            st.write(\"---\")\n",
        "            st.write(f\"**Probable Target Audience:** {st.session_state['inferred_audience']}\")\n",
        "            st.write(f\"**Probable Appeal:** {st.session_state['probable_appeal']}\")\n",
        "            st.write(\"---\")\n",
        "\n",
        "            # Display detailed rhetoric narration classification results\n",
        "            st.write(\"**Rhetoric Narration Classification Scores:**\")\n",
        "            rhetoric_df_display = pd.DataFrame({\n",
        "                'Rhetoric Bin': st.session_state['classification_results']['labels'],\n",
        "                'Confidence Score': [f\"{score:.4f}\" for score in st.session_state['classification_results']['scores']]\n",
        "            })\n",
        "            st.dataframe(rhetoric_df_display)\n",
        "            st.write(\"---\")\n",
        "\n",
        "            # Display detailed appeal type classification results\n",
        "            st.write(\"**Appeal Type Classification Scores:**\")\n",
        "            appeal_df_display = pd.DataFrame({\n",
        "                'Appeal Type': st.session_state['appeal_classification_results']['labels'],\n",
        "                'Confidence Score': [f\"{score:.4f}\" for score in st.session_state['appeal_classification_results']['scores']]\n",
        "            })\n",
        "            st.dataframe(appeal_df_display)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"An error occurred while processing the URL: {e}\")\n",
        "    else:\n",
        "        st.warning(\"Please enter a search query or an article URL.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b303bb9a"
      },
      "source": [
        "## Update `requirements.txt` (if necessary)\n",
        "\n",
        "### Subtask:\n",
        "Ensure all newly used libraries (like `newspaper3k`) are included in `requirements.txt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6afe5ffb"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the `requirements.txt` file to include `newspaper3k` and `lxml_html_clean`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07b30ed6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7263ef-77c1-49c5-97f5-f2738e04fa91"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "newsapi-python\n",
        "transformers\n",
        "matplotlib\n",
        "pandas\n",
        "newspaper3k\n",
        "lxml_html_clean"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20fb84be"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The Streamlit application (`app.py`) was successfully modified to include a text input field for a URL and an \"Analyze\" button.\n",
        "*   Integration with `newspaper3k` was successfully implemented to download and parse article content from a provided URL.\n",
        "*   A zero-shot classification pipeline using the `facebook/bart-large-mnli` model from the `transformers` library was integrated to classify the extracted article text against predefined rhetoric narration bins and appeal types (Ethos, Pathos, Logos).\n",
        "*   A rule-based logic was implemented to infer the probable target audience and probable appeal based on the top classification scores from the rhetoric narration and appeal type classifications, using a confidence threshold of 0.7.\n",
        "*   The analysis results, including the extracted text, inferred audience, probable appeal, and detailed classification scores for both rhetoric narration and appeal types, are formatted and displayed in the Streamlit app using dataframes.\n",
        "*   The `requirements.txt` file was updated to include all necessary libraries: `streamlit`, `newsapi-python`, `transformers`, `matplotlib`, `pandas`, `newspaper3k`, and `lxml_html_clean`.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current audience and appeal inference is based on simple rules and the top-scoring categories. Future iterations could explore more sophisticated methods, such as considering combinations of rhetoric devices or using a dedicated model for audience prediction.\n",
        "*   Adding visualizations (e.g., bar charts of classification scores) could enhance the clarity and interpretability of the rhetoric analysis results for the user.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-sii8Exqjby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94d55886"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1becd438",
        "outputId": "014880ca-8051-4236-8e6a-164625c37c5c"
      },
      "source": [
        "# Run the Streamlit app in the foreground to see the output and access the app\n",
        "!streamlit run app.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8505\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8505\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.230.42.18:8505\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install streamlit transformers newspaper3k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7RcGUdZ0CC_",
        "outputId": "25c59044-f9c3-4609-c741-7e6584aee886"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.47.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.48.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.7.14)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e09a30e",
        "outputId": "97f25d15-8549-469d-985c-9f584604c95b"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "print(\"Click the link to access your Streamlit app:\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click the link to access your Streamlit app:\n"
          ]
        }
      ]
    }
  ]
}